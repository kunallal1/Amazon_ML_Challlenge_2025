{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a201a88",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub\n",
    "!hf download distilbert-base-uncased --local-dir /kaggle/working/distilbert-base-uncased --quiet\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Hybrid DistilBERT + Tabular Price Prediction (v3)\n",
    "# Using Optuna-tuned hyperparameters\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel, get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch import amp\n",
    "\n",
    "# -------------------------------\n",
    "# Reproducibility\n",
    "# -------------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# -------------------------------\n",
    "# Load tokenizer/model locally\n",
    "# -------------------------------\n",
    "LOCAL_BERT_DIR = \"./distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(LOCAL_BERT_DIR)\n",
    "bert_model = DistilBertModel.from_pretrained(LOCAL_BERT_DIR)\n",
    "print(\"✅ DistilBERT loaded locally.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Paths\n",
    "# -------------------------------\n",
    "TRAIN_PATH = \"/kaggle/input/mldataset/dataset/train.csv\"\n",
    "TEST_PATH  = \"/kaggle/input/mldataset/dataset/test.csv\"\n",
    "\n",
    "# -------------------------------\n",
    "# SMAPE + LogCosh loss\n",
    "# -------------------------------\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6, alpha=0.7):\n",
    "        super().__init__()\n",
    "        self.eps, self.alpha = eps, alpha\n",
    "    def forward(self, y_pred, y_true):\n",
    "        smape = torch.mean(torch.abs(y_pred - y_true) /\n",
    "                           ((torch.abs(y_true) + torch.abs(y_pred) + self.eps) / 2))\n",
    "        logcosh = torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
    "        return self.alpha * smape + (1 - self.alpha) * logcosh\n",
    "\n",
    "# -------------------------------\n",
    "# Data preprocessing\n",
    "# -------------------------------\n",
    "def extract_value_unit(text):\n",
    "    value_match = re.search(r'Value:\\s*([0-9\\.]+)', str(text))\n",
    "    unit_match  = re.search(r'Unit:\\s*([A-Za-z0-9 %]+)', str(text))\n",
    "    value = float(value_match.group(1)) if value_match else 0.0\n",
    "    unit  = unit_match.group(1).strip() if unit_match else 'Unknown'\n",
    "    return value, unit\n",
    "\n",
    "def clean_text(text):\n",
    "    t = str(text).lower()\n",
    "    t = re.sub(r'[^a-z0-9.% ]', ' ', t)\n",
    "    t = re.sub(r'\\s+', ' ', t).strip()\n",
    "    return t\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH).fillna('')\n",
    "test_df  = pd.read_csv(TEST_PATH).fillna('')\n",
    "train_df[['Value','Unit']] = train_df['catalog_content'].apply(lambda x: pd.Series(extract_value_unit(x)))\n",
    "test_df[['Value','Unit']]  = test_df['catalog_content'].apply(lambda x: pd.Series(extract_value_unit(x)))\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    df['text'] = df['catalog_content'].apply(clean_text)\n",
    "    df['text_len'] = df['text'].apply(len)\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "    df['has_image'] = df['image_link'].apply(lambda x: 1 if isinstance(x, str) and len(x)>5 else 0)\n",
    "    df['Value_log'] = np.log1p(df['Value'])\n",
    "    df['value_unit_ratio'] = df['Value_log'] / (df['word_count']+1)\n",
    "    df['text_len_per_word'] = df['text_len'] / (df['word_count']+1)\n",
    "    df['is_short_text'] = (df['word_count'] < 5).astype(int)\n",
    "\n",
    "num_features = ['Value_log','text_len','word_count','has_image',\n",
    "                'value_unit_ratio','text_len_per_word','is_short_text']\n",
    "scaler = StandardScaler()\n",
    "X_num_train = scaler.fit_transform(train_df[num_features])\n",
    "X_num_test  = scaler.transform(test_df[num_features])\n",
    "\n",
    "unit2idx = {u:i for i,u in enumerate(train_df['Unit'].unique())}\n",
    "train_df['Unit_idx'] = train_df['Unit'].map(unit2idx).fillna(len(unit2idx)).astype(int)\n",
    "test_df['Unit_idx']  = test_df['Unit'].map(unit2idx).fillna(len(unit2idx)).astype(int)\n",
    "unit_vocab_size = len(unit2idx)\n",
    "\n",
    "# -------------------------------\n",
    "# Dataset\n",
    "# -------------------------------\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, texts, num_feats, unit_idx, targets=None, tokenizer=None, max_len=128, aug=False):\n",
    "        self.num_feats = torch.tensor(num_feats, dtype=torch.float32)\n",
    "        self.unit_idx  = torch.tensor(unit_idx, dtype=torch.long)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1) if targets is not None else None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.texts = texts\n",
    "        if aug:\n",
    "            self.texts = [self._augment_text(t) for t in texts]\n",
    "        toks = self.tokenizer(self.texts, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')\n",
    "        self.input_ids, self.attention_mask = toks['input_ids'], toks['attention_mask']\n",
    "\n",
    "    def _augment_text(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) > 5 and random.random() < 0.3:\n",
    "            drop_idx = random.choice(range(len(words)))\n",
    "            words.pop(drop_idx)\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def __len__(self): return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'num_feats': self.num_feats[idx],\n",
    "            'unit_idx': self.unit_idx[idx]\n",
    "        }\n",
    "        if self.targets is not None:\n",
    "            item['target'] = self.targets[idx]\n",
    "        return item\n",
    "\n",
    "# -------------------------------\n",
    "# Train/val split\n",
    "# -------------------------------\n",
    "y_all = np.log1p(train_df['price'].values).astype(np.float32)\n",
    "train_idx, val_idx = train_test_split(np.arange(len(train_df)), test_size=0.1, random_state=42)\n",
    "\n",
    "train_ds = PriceDataset(train_df['text'].iloc[train_idx].tolist(), X_num_train[train_idx],\n",
    "                        train_df['Unit_idx'].iloc[train_idx].values, y_all[train_idx], tokenizer, aug=True)\n",
    "val_ds = PriceDataset(train_df['text'].iloc[val_idx].tolist(), X_num_train[val_idx],\n",
    "                      train_df['Unit_idx'].iloc[val_idx].values, y_all[val_idx], tokenizer)\n",
    "test_ds = PriceDataset(test_df['text'].tolist(), X_num_test, test_df['Unit_idx'].values, tokenizer=tokenizer)\n",
    "\n",
    "# Optuna best params\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Model with Optuna-tuned params\n",
    "# -------------------------------\n",
    "class HybridPriceModel(nn.Module):\n",
    "    def __init__(self, bert_model, num_feat_dim, unit_vocab_size,\n",
    "                 emb_dim=11, dropout=0.3740734052386133, hidden1=808, hidden2=272):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.unit_emb = nn.Embedding(unit_vocab_size+1, emb_dim)\n",
    "        bert_dim = bert_model.config.dim\n",
    "        total_dim = bert_dim*4 + num_feat_dim + emb_dim\n",
    "        self.attn_vec = nn.Linear(bert_dim, 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(total_dim, hidden1), nn.LayerNorm(hidden1), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden1, hidden2), nn.LayerNorm(hidden2), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden2, 128), nn.LayerNorm(128), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, num_feats, unit_idx):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        mean_pool = (out * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
    "        max_pool,_ = (out * mask).max(1)\n",
    "        cls = out[:,0,:]\n",
    "\n",
    "        attn_logits = self.attn_vec(out).squeeze(-1).float()\n",
    "        attn_logits = attn_logits.masked_fill(attention_mask == 0, -1e9)\n",
    "        attn_w = torch.softmax(attn_logits, dim=1)\n",
    "        attn_w = attn_w.to(out.dtype).unsqueeze(1)\n",
    "        attn_pool = torch.bmm(attn_w, out).squeeze(1)\n",
    "\n",
    "        x = torch.cat([cls, mean_pool, max_pool, attn_pool, num_feats, self.unit_emb(unit_idx)], dim=1)\n",
    "        return self.head(x)\n",
    "\n",
    "# -------------------------------\n",
    "# Training setup\n",
    "# -------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = HybridPriceModel(bert_model, X_num_train.shape[1], unit_vocab_size).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "bert_params = [p for n,p in model.named_parameters() if 'bert' in n]\n",
    "head_params = [p for n,p in model.named_parameters() if 'bert' not in n]\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {'params': bert_params, 'lr': 5.545732304370043e-06, 'weight_decay': 1e-4},\n",
    "    {'params': head_params, 'lr': 0.004846331266624619, 'weight_decay': 1e-4}\n",
    "])\n",
    "\n",
    "EPOCHS = 8\n",
    "total_steps = EPOCHS * len(train_loader)\n",
    "warmup_steps = max(1, int(0.1 * total_steps))\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "criterion = HybridLoss()\n",
    "scaler = amp.GradScaler() if device == \"cuda\" else None\n",
    "\n",
    "best_val = np.inf\n",
    "patience = 4\n",
    "early_stop_count = 0\n",
    "\n",
    "# -------------------------------\n",
    "# Training loop\n",
    "# -------------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        num_feats = batch['num_feats'].to(device)\n",
    "        unit_idx  = batch['unit_idx'].to(device)\n",
    "        targets = batch['target'].to(device)\n",
    "\n",
    "        if scaler is not None:\n",
    "            with amp.autocast(device_type='cuda'):\n",
    "                preds = model(input_ids, attention_mask, num_feats, unit_idx)\n",
    "                loss = criterion(preds, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            preds = model(input_ids, attention_mask, num_feats, unit_idx)\n",
    "            loss = criterion(preds, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item() * input_ids.size(0)\n",
    "        pbar.set_postfix({'batch_loss': f\"{loss.item():.6f}\"})\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for b in val_loader:\n",
    "            ids = b['input_ids'].to(device)\n",
    "            mask = b['attention_mask'].to(device)\n",
    "            numf = b['num_feats'].to(device)\n",
    "            unit = b['unit_idx'].to(device)\n",
    "            tgt = b['target'].to(device)\n",
    "            if scaler is not None:\n",
    "                with amp.autocast(device_type='cuda'):\n",
    "                    out = model(ids, mask, numf, unit)\n",
    "            else:\n",
    "                out = model(ids, mask, numf, unit)\n",
    "            val_loss_acc += criterion(out, tgt).item() * ids.size(0)\n",
    "\n",
    "    val_loss = val_loss_acc / len(val_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.6f} - Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        early_stop_count = 0\n",
    "        save_path = \"best_model.pth\"\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            torch.save(model.module.state_dict(), save_path)\n",
    "        else:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        print(f\"✅ Saved best model -> {save_path} | Val Loss: {best_val:.6f}\")\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "        if early_stop_count >= patience:\n",
    "            print(\"⏹️ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# -------------------------------\n",
    "# Load best model\n",
    "# -------------------------------\n",
    "if isinstance(model, nn.DataParallel):\n",
    "    model.module.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# Prediction\n",
    "# -------------------------------\n",
    "preds_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        num_feats = batch['num_feats'].to(device)\n",
    "        unit_idx  = batch['unit_idx'].to(device)\n",
    "        if scaler is not None:\n",
    "            with amp.autocast(device_type='cuda'):\n",
    "                out = model(input_ids, attention_mask, num_feats, unit_idx)\n",
    "        else:\n",
    "            out = model(input_ids, attention_mask, num_feats, unit_idx)\n",
    "        out = out.cpu().numpy().flatten()\n",
    "        preds_list.append(out)\n",
    "\n",
    "preds = np.concatenate(preds_list, axis=0)\n",
    "preds = np.expm1(preds)\n",
    "preds = np.clip(preds, 0, None)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': test_df['sample_id'],\n",
    "    'price': preds\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"✅ Saved submission.csv | Best Val Loss: {best_val:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
